{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Python\n",
    "\n",
    "## Regularization in regression using Python\n",
    "\n",
    "### What is linear regression?\n",
    "Linear Regression attempts to find a relationship between a dependent variable and one or more explantory (or independent) variables. In case of simple linear regression where there is only one explanatory variable, linear regression is described as a general equation as shown below.\n",
    "\n",
    "\\begin{align}\n",
    "y = b_0 + b_1x + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "where y is the dependent variable, x is the explanatory variable with coefficient b<sub>1</sub>, and b<sub>0</sub> is the intercept with the error term $\\epsilon$.\n",
    "\n",
    "For **multiple linear regression**, we have two or more explanatory variables so the equation. It is described as a general equation as shown below. \n",
    "\n",
    "\\begin{align}\n",
    "y = b_0 + b_1x_1 + ... + b_px_p + \\epsilon \n",
    "\\end{align}\n",
    "\n",
    "where y is the dependent variable, x<sub>1</sub>...x<sub>p</sub> are the explanatory variables with regression coefficients b<sub>1</sub>...b<sub>p</sub>, and b<sub>0</sub> is the intercept with the error term $\\epsilon$.\n",
    "\n",
    "### What is regularization?\n",
    "Regularization is a kind of regression that shrinks the coefficient estimates towards zero. This techniques discourages formation of a complex model, so as to avoid risk of overfitting.\n",
    "\n",
    "#### Underfitting and Overfitting\n",
    "* Underfitting occurs when a model is not able to capture the underlying trend of the data.\n",
    "* Overfitting occurs when a model follows the trend of training data very closely but is not able to replicate the same performance on testing data.\n",
    "\n",
    "A good fit model generalizes well and neither underfits nor overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import probplot\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression, RFE, SelectKBest\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "This is a simulated dataset made for teaching regression containing 10 independent variables named **Feat01, Feat02..., Feat10** and a dependent variable called **Target**. \n",
    "\n",
    "### The task\n",
    "The task at hand is to predict the **Target** variable such that the values of predictions are as close to the actual values as possible. This will be done using a multiple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feat01</th>\n",
       "      <th>Feat02</th>\n",
       "      <th>Feat03</th>\n",
       "      <th>Feat04</th>\n",
       "      <th>Feat05</th>\n",
       "      <th>Feat06</th>\n",
       "      <th>Feat07</th>\n",
       "      <th>Feat08</th>\n",
       "      <th>Feat09</th>\n",
       "      <th>Feat10</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11617.463</td>\n",
       "      <td>135.935905</td>\n",
       "      <td>10960</td>\n",
       "      <td>491.597396</td>\n",
       "      <td>80.714963</td>\n",
       "      <td>0.675678</td>\n",
       "      <td>16.891946</td>\n",
       "      <td>0.506758</td>\n",
       "      <td>1.198996</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1086607.005</td>\n",
       "      <td>1983.900611</td>\n",
       "      <td>13065</td>\n",
       "      <td>1391.028397</td>\n",
       "      <td>64.221576</td>\n",
       "      <td>0.880523</td>\n",
       "      <td>0.146754</td>\n",
       "      <td>0.146754</td>\n",
       "      <td>1.230974</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>37945.952</td>\n",
       "      <td>2726.252884</td>\n",
       "      <td>8772</td>\n",
       "      <td>222.952910</td>\n",
       "      <td>37.589307</td>\n",
       "      <td>0.325336</td>\n",
       "      <td>16.266790</td>\n",
       "      <td>0.488004</td>\n",
       "      <td>1.208015</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>79471.821</td>\n",
       "      <td>3448.065646</td>\n",
       "      <td>36414</td>\n",
       "      <td>883.345912</td>\n",
       "      <td>662.170505</td>\n",
       "      <td>0.257565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128782</td>\n",
       "      <td>1.256901</td>\n",
       "      <td>867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5486.580</td>\n",
       "      <td>1509.753541</td>\n",
       "      <td>13040</td>\n",
       "      <td>742.253326</td>\n",
       "      <td>99.315014</td>\n",
       "      <td>0.129816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129816</td>\n",
       "      <td>1.255410</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feat01       Feat02       Feat03  Feat04       Feat05      Feat06  \\\n",
       "0       0    11617.463   135.935905   10960   491.597396   80.714963   \n",
       "1       0  1086607.005  1983.900611   13065  1391.028397   64.221576   \n",
       "2       0    37945.952  2726.252884    8772   222.952910   37.589307   \n",
       "3       0    79471.821  3448.065646   36414   883.345912  662.170505   \n",
       "4       1     5486.580  1509.753541   13040   742.253326   99.315014   \n",
       "\n",
       "     Feat07     Feat08    Feat09    Feat10  Target  \n",
       "0  0.675678  16.891946  0.506758  1.198996     137  \n",
       "1  0.880523   0.146754  0.146754  1.230974     335  \n",
       "2  0.325336  16.266790  0.488004  1.208015     172  \n",
       "3  0.257565   0.000000  0.128782  1.256901     867  \n",
       "4  0.129816   0.000000  0.129816  1.255410     815  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Target\"],axis=1) # Explanatory(independent) variables\n",
    "y = data[\"Target\"] # Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "Data is divided into training and test set. Training set is used to train the linear model and test set is used evaluate the trained model. Here, we are using 70% data in the training set, leaving 30% data for the test set. The train-test set ratio is determined keeping in mind the size of data. Larger test sets (around 20-30% data) provide more reliable evaluation results but at the same time training set should be enough so that underfitting does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=100,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear regression model\n",
    "We will train a linear regression model using all ten available features. The `fit` method of the `LinearRegression()` class trains models and estimates the best possible intercept and coefficient(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a linear model for multiple linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature/Column</th>\n",
       "      <th>Coef_linear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feat01</td>\n",
       "      <td>2.319425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feat02</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feat03</td>\n",
       "      <td>0.006078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feat04</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feat05</td>\n",
       "      <td>0.013329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feat06</td>\n",
       "      <td>0.111174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feat07</td>\n",
       "      <td>0.511887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feat08</td>\n",
       "      <td>-0.123155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feat09</td>\n",
       "      <td>-16.116907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feat10</td>\n",
       "      <td>11177.060571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature/Column   Coef_linear\n",
       "0         Feat01      2.319425\n",
       "1         Feat02      0.000010\n",
       "2         Feat03      0.006078\n",
       "3         Feat04      0.001139\n",
       "4         Feat05      0.013329\n",
       "5         Feat06      0.111174\n",
       "6         Feat07      0.511887\n",
       "7         Feat08     -0.123155\n",
       "8         Feat09    -16.116907\n",
       "9         Feat10  11177.060571"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The coefficients for each columns\n",
    "model_coefs = pd.DataFrame({'Feature/Column':list(X_train.columns),\"Coef_linear\":model.coef_})\n",
    "model_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13397.877366112254"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model's intercept\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking predictions\n",
    "We can use `predict` method to take predictions of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "We will be evaluating the model now. We will start by using the metrics R-squared score and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score is 0.8978\n",
      "The Root Mean Squared error is 82.8497\n"
     ]
    }
   ],
   "source": [
    "print(\"The R-squared score is {:.4f}\".format(r2_score(y_test,y_pred)))\n",
    "print(\"The Root Mean Squared error is {:.4f}\".format(np.sqrt(mean_squared_error(y_test,y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3-fold CV R^2 scores are [0.89411875 0.89787452 0.89275597] \n",
      "with a mean R^2 score of 0.8949\n",
      "The 3-fold CV RMSE scores are [85.04847542209288, 83.56004204324616, 84.72165770028043] \n",
      "with a mean RMSE of 84.4434\n"
     ]
    }
   ],
   "source": [
    "r2_cross_val = cross_val_score(LinearRegression(),X,y,cv=3,scoring=\"r2\")\n",
    "print(\"The 3-fold CV R^2 scores are {} \\nwith a mean R^2 score of {:.4f}\".format(r2_cross_val,np.mean(r2_cross_val)))\n",
    "rmse_cross_val = cross_val_score(LinearRegression(),X,y,cv=3,scoring=\"neg_root_mean_squared_error\")\n",
    "print(\"The 3-fold CV RMSE scores are {} \\nwith a mean RMSE of {:.4f}\".format([-i for i in rmse_cross_val],-np.mean(rmse_cross_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Ridge regression or L2 regularization brings values of coefficients near zero to enforce regularization. \n",
    "\n",
    "Penalty is described by $\\lambda$ parameter. More the value of $\\lambda$, lesser the flexibility. For low values of $\\lambda$, the coefficients are very similar to that of a multiple linear regression model. As $\\lambda$ increases, the differences between the results of Ridge model and linear regression model increase. \n",
    "\n",
    "Below is a demonstration where we use increasing values of $\\lambda$ iteratively to show the effect of Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At alpha = 0.0001\n",
      "The R-squared score is 0.8978\n",
      "The Root Mean Squared error is 82.8499\n",
      "__________\n",
      "At alpha = 0.001\n",
      "The R-squared score is 0.8978\n",
      "The Root Mean Squared error is 82.8516\n",
      "__________\n",
      "At alpha = 0.01\n",
      "The R-squared score is 0.8977\n",
      "The Root Mean Squared error is 82.8773\n",
      "__________\n",
      "At alpha = 0.1\n",
      "The R-squared score is 0.8952\n",
      "The Root Mean Squared error is 83.8777\n",
      "__________\n",
      "At alpha = 1\n",
      "The R-squared score is 0.8293\n",
      "The Root Mean Squared error is 107.0405\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.0001,0.001,0.01,0.1,1]\n",
    "for i in alpha_values:\n",
    "    ridge = Ridge(alpha=i)\n",
    "    ridge.fit(X_train,y_train)\n",
    "    model_coefs[\"Coef_ridge_alpha{}\".format(i)] = ridge.coef_\n",
    "    y_pred_new = ridge.predict(X_test)\n",
    "    print(\"At alpha = {}\".format(i))\n",
    "    print(\"The R-squared score is {:.4f}\".format(r2_score(y_test,y_pred_new)))\n",
    "    print(\"The Root Mean Squared error is {:.4f}\".format(np.sqrt(mean_squared_error(y_test,y_pred_new))))\n",
    "    print(\"__________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef_linear</th>\n",
       "      <th>Coef_ridge_alpha0.0001</th>\n",
       "      <th>Coef_ridge_alpha0.001</th>\n",
       "      <th>Coef_ridge_alpha0.01</th>\n",
       "      <th>Coef_ridge_alpha0.1</th>\n",
       "      <th>Coef_ridge_alpha1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature/Column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Feat01</th>\n",
       "      <td>2.319425</td>\n",
       "      <td>2.319599</td>\n",
       "      <td>2.321161</td>\n",
       "      <td>2.336635</td>\n",
       "      <td>2.477652</td>\n",
       "      <td>3.195780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat02</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat03</th>\n",
       "      <td>0.006078</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>0.007603</td>\n",
       "      <td>0.014519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat04</th>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.002635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat05</th>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013378</td>\n",
       "      <td>0.013818</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.038205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat06</th>\n",
       "      <td>0.111174</td>\n",
       "      <td>0.111202</td>\n",
       "      <td>0.111457</td>\n",
       "      <td>0.113988</td>\n",
       "      <td>0.137046</td>\n",
       "      <td>0.254387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat07</th>\n",
       "      <td>0.511887</td>\n",
       "      <td>0.505858</td>\n",
       "      <td>0.451653</td>\n",
       "      <td>0.085146</td>\n",
       "      <td>4.975513</td>\n",
       "      <td>29.790550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat08</th>\n",
       "      <td>0.123155</td>\n",
       "      <td>0.123240</td>\n",
       "      <td>0.124001</td>\n",
       "      <td>0.131536</td>\n",
       "      <td>0.200201</td>\n",
       "      <td>0.549791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat09</th>\n",
       "      <td>16.116907</td>\n",
       "      <td>16.128023</td>\n",
       "      <td>16.227966</td>\n",
       "      <td>17.217670</td>\n",
       "      <td>26.229101</td>\n",
       "      <td>71.682119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feat10</th>\n",
       "      <td>11177.060571</td>\n",
       "      <td>11175.960175</td>\n",
       "      <td>11166.066357</td>\n",
       "      <td>11068.083322</td>\n",
       "      <td>10175.213750</td>\n",
       "      <td>5632.269492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Coef_linear  Coef_ridge_alpha0.0001  Coef_ridge_alpha0.001  \\\n",
       "Feature/Column                                                                \n",
       "Feat01              2.319425                2.319599               2.321161   \n",
       "Feat02              0.000010                0.000010               0.000010   \n",
       "Feat03              0.006078                0.006080               0.006095   \n",
       "Feat04              0.001139                0.001139               0.001142   \n",
       "Feat05              0.013329                0.013334               0.013378   \n",
       "Feat06              0.111174                0.111202               0.111457   \n",
       "Feat07              0.511887                0.505858               0.451653   \n",
       "Feat08              0.123155                0.123240               0.124001   \n",
       "Feat09             16.116907               16.128023              16.227966   \n",
       "Feat10          11177.060571            11175.960175           11166.066357   \n",
       "\n",
       "                Coef_ridge_alpha0.01  Coef_ridge_alpha0.1  Coef_ridge_alpha1  \n",
       "Feature/Column                                                                \n",
       "Feat01                      2.336635             2.477652           3.195780  \n",
       "Feat02                      0.000010             0.000012           0.000022  \n",
       "Feat03                      0.006244             0.007603           0.014519  \n",
       "Feat04                      0.001169             0.001409           0.002635  \n",
       "Feat05                      0.013818             0.017823           0.038205  \n",
       "Feat06                      0.113988             0.137046           0.254387  \n",
       "Feat07                      0.085146             4.975513          29.790550  \n",
       "Feat08                      0.131536             0.200201           0.549791  \n",
       "Feat09                     17.217670            26.229101          71.682119  \n",
       "Feat10                  11068.083322         10175.213750        5632.269492  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_alpha_comparison = model_coefs.set_index(\"Feature/Column\").apply(lambda x: np.abs(x),axis=1)\n",
    "ridge_alpha_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17532d6d448>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(ridge_alpha_comparison.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the coefficient of strong regressor Feat10, the coefficients of other regressors have actually increased. So, there is an overall decrease in coefficient values but on an individual level, only Feat10's coefficient decreases constantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At alpha = 0.1 , the 3-fold CV R^2 scores are [0.89168758 0.89639056 0.88963433] \n",
      "with a mean R^2 score of 0.8926\n",
      "At alpha = 0.1 , the 3-fold CV RMSE scores are [86.01934616707074, 84.16494942320219, 85.94584379063265] \n",
      "with a mean RMSE of 85.3767\n"
     ]
    }
   ],
   "source": [
    "r2_cross_val = cross_val_score(Ridge(alpha=0.1),X,y,cv=3,scoring=\"r2\")\n",
    "print(\"At alpha = 0.1 , the 3-fold CV R^2 scores are {} \\nwith a mean R^2 score of {:.4f}\".format(r2_cross_val,np.mean(r2_cross_val)))\n",
    "rmse_cross_val = cross_val_score(Ridge(alpha=0.1),X,y,cv=3,scoring=\"neg_root_mean_squared_error\")\n",
    "print(\"At alpha = 0.1 , the 3-fold CV RMSE scores are {} \\nwith a mean RMSE of {:.4f}\".format([-i for i in rmse_cross_val],-np.mean(rmse_cross_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At $\\lambda$ = 0.1, we have sacrificed negligible performance but the model is less likely to overfit.\n",
    "\n",
    "**Conclusion:** As the coefficients of almost all variables were close to zero, not much decrease in value of coefficients was seen for variables other than Feat10. This might have happened due to large coefficient of Feat10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
